\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\setlength{\textwidth}{160mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\textheight}{250mm}
\setlength{\voffset}{-20mm}
%\usepackage{showframe}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{fancyvrb}
\usepackage{xspace}

\usepackage{minted, xcolor}
\usemintedstyle{lovelace}
\definecolor{bg}{rgb}{0.95,0.95,0.95}
\definecolor{bg_shell}{rgb}{0.95,0.95,1.00}
\definecolor{bg_shell2}{rgb}{0.95,1.00,0.95}
\definecolor{shadecolor}{rgb}{0.96,0.96,0.96} % for snugshade
\newminted[python]{python}{bgcolor=bg}
\newminted[shell]{bash}{bgcolor=bg_shell}
\newminted[shell2]{bash}{bgcolor=bg_shell2}



\newcommand{\pyTrue}{\mintinline{python}/True/\xspace}
\newcommand{\pyFalse}{\mintinline{python}/False/\xspace}


\title{\vspace{-20mm}Expertmaker Accelerator\\[1ex]\Large{Quick Install using the ``Project Skeleton'' Repository + Examples}}
\date{v2, 2018-05-30, Added performance example.  See page \pageref{example:datasetperf}.}
\begin{document}
\maketitle

\section*{Introduction and System Requirements}
The \texttt{accelerator-project\_skeleton} project provides a simple
and convenient way to install the Accelerator locally.  This document
lists the necessary steps to set up the Accelerator using it.  The
last part of the document is dedicated to examples showing important
Accelerator concepts.

The Accelerator will run on almost any hardware, from small laptops to
large multi-CPU rack servers.  It is assumed in this manual that the
computer is running Ubuntu~16.04~LTS or Debian 9.  The Accelerator
team is actively testing on Ubuntu, Debian, and FreeBSD, but the
Accelerator will most likely run on many other Linux distributions as well.


\section*{Installation}
There are three steps in the installation: \textsl{resolve dependencies}, \textsl{clone
repository}, and \textsl{run the initiation script}.  These steps will be
described next.
\subsection*{1. Dependencies}
The first step is to make sure that all software package dependencies
are met.  This command will install all required packages
\begin{shell}
sudo apt-get install build-essential python-dev python3-dev zlib1g-dev git virtualenv
\end{shell}
The installer requires only \texttt{git}, \texttt{virtualenv}, and
some \texttt{dev} packages in order to compile C-code.

\subsection*{2. Clone Repository}
Clone the \texttt{accelerator-project\_skeleton} like this
\begin{shell}
git clone https://github.com/eBay/accelerator-project_skeleton.git
\end{shell}

\subsection*{3. Setup}
The Accelerator will now be installed \textsl{locally without any
  administrator privileges}.  To continue, \texttt{cd} into the cloned
directory
\begin{shell}
cd accelerator-project_skeleton
\end{shell}
In this directory there is a file \texttt{init.py} that performs all
the installation steps.  It will work out-of-the-box, but for a
customized install it is recommended to read and modify this file
before continuing.  The next step is to run the script
\begin{shell}
./init.py
\end{shell}
This will do a complete setup, and the next section provides more
information about the process.  After the script is finished, the
Accelerator installation is complete.  It could be run by issuing
\begin{shell}
cd accelerator
./daemon.py  
\end{shell}
The first time the Accelerator is run, it will compile some functions
written in the C programming language.  On some systems, this process
may generate a few warning messages, but that is okay.  Setup is now
complete.

\section*{Accessing Libraries in the Virtual Environment}
Since the installer uses a virtual environment for installing
packages, it must be initiated in all shells running for example
\texttt{automatarunner} or \texttt{dsinfo}.  This is done by issuing
\begin{shell}
source ../venv/py3/bin/activate
\end{shell}
from the \texttt{accelerator} directory.



\section*{The Installation in more Detail}
The \texttt{accelerator-project\_skeleton} script \texttt{init.py}
will setup virtual environments for Python2 and Python3 using
\texttt{virtualenv}.  In these virtual environments, it will download
and install some depending packages, and \texttt{git clone} and
install the \texttt{accelerator-gzutil} library.  The Accelerator
itself is \texttt{git clone}d into a git submodule in the
\texttt{accelerator} directory.

The default configuration file is located in
\texttt{conf/framework.conf}.  This file is used to specify workdirs,
method directories, and more.  For more information, see the
Accelerator User's Reference Manual.


\vfill
\subsection*{Acknowledgments}
Thanks to Stefan H{\aa}konsson for suggestions, testing, and proof reading.


\section*{References}
\texttt{https://berkeman.github.io/pdf/acc\_manual.pdf}



\clearpage
\section{Example:  Dataset Performance Measurement}
\label{example:datasetperf}

\begin{figure}[h]\centering
  \input{figures/example_perf.pdftex_t}
  \label{fig:ex2}
\end{figure}

This example will create one billion lines of random data and use it
in a set of computations.  By looking at the execution times, stored
by default by the Accelerator, the example can be used as a simple
performance measurement of a machine.  The size of the dataset is an
input parameter, and by default, a much smaller set of data is created
in order to save space and execution time.  At the end of the chapter,
we present some quite impressive results.

\subsection*{Example Data Flow}
The figure above shows the data flow in the example.  The following happens
\begin{enumerate}
\item[(1)] The example starts by creating 100 chained datasets, each
  one containing 10 millon lines.  (Default setting is actually only
  1\% of this, see section on changing dataset size.)  The datasets
  will have the following columns
  \begin{snugshade}
  \noindent\begin{tabular}{@{}p{3.0cm}p{2cm}p{8.5cm}}\hline\\[-1ex]
  \textbf{name} & \textbf{type} & \textbf{description}\\[1ex]
  \hline\\[-0.5ex]
  \texttt{a string}      & \texttt{ascii}   & a short random string, 10 characters long\\
  \texttt{large number}  & \texttt{number}  & an integer evenly spread in the range $[-10^{20}, 10^{20}]$ \\
  \texttt{small number}  & \texttt{number}  & an integer evenly spread in the range $[-99, 99]$\\
  \texttt{small integer} & \texttt{int32}   & the same integer as in the previous column\\
  \texttt{gauss number}  & \texttt{number}  & a normalised gaussian distribution\\
  \texttt{gauss float}   & \texttt{float64} & the same float as the previous column\\[1ex]
  \hline
  \end{tabular}
  \end{snugshade}
  Stored as a CSV (Comma Separated Values) file, this corresponds to
  79 bytes per line on average.
    
  \item[(2)] The complete dataset chain will be exported to a
    compressed text file in CSV-format.  A one billion line CVS file
    is about 36GiB compressed and 79GiB uncompressed.
    
  \item[(3)] The previously stored CSV file is imported again into a
    single dataset.  Note that the import job will find and name the
    columns, and type all data to the \texttt{bytes} type.

  \item[(4)] The imported dataset is typed, i.e.\ the data is written
    to disk in a more efficient format together with meta information
    such as \texttt{min} and \texttt{max} values.
    
  \item[(5)]  Finally, a number of operations are performed on the
    individual columns.  The operation are
    \begin{itemize}
    \item[A.] Sum all values of the column.
    \item[B.] Sum all \textsl{positive} values of the column.
      Implemented using the iterator's \texttt{range}-argument.
    \item[C.] Create histograms of the column's data.
    \item[D.] Find a substring ``\texttt{ExAx}'' in the string column
      and count number of matching lines.

    \end{itemize}

\end{enumerate}


\subsection*{Running the Example}
The example is located in the \texttt{example\_perf} directory.  Here
is a list of steps showing how to run it:

\begin{enumerate}
\item Clone and setup the \texttt{project\_skeleton} as described
  earlier in this document
\begin{shell}
git clone https://github.com/eBay/accelerator-project_skeleton.git
cd accelerator-project_skeleton
./init.py
\end{shell}

\item Now we can start the Accelerator.  To start the server type
  \begin{shell}
cd accelerator
./daemon.py
  \end{shell}
  This terminal is now our \textsl{server} terminal, displaying the
  Accelerator's \texttt{stdout} and \texttt{stderr}.
  
\item Next, open a second terminal emulator for the \textsl{client}
  side.  The Accelerator team prefers using GNU \texttt{screen}, but any
  terminal emulator like \texttt{xterm} is fine.

  In this second terminal (green), \texttt{cd} to the
  \texttt{accelerator} directory
\begin{shell2}
cd accelerator
\end{shell2}
Since the installation is local (no administration privileges used),
using virtual environments, we need to set up the virtual environment
in this terminal like this
\begin{shell2}
source ../venv/py3/bin/activate
\end{shell2}
Now we can run the the build script.
\begin{shell2}
./automatarunner -P example_perf
\end{shell2}
While running, the build script will print its progress to the
terminal.  The output is quite verbose so it might be helpful to read
it.  The Accelerator \textsl{server} will also print information
during execution.  Just switch to the terminal running
\texttt{daemon.py} to see it.

Please read the section below on the dataset size.  Also note that all
generated data will by default be stored in the home directory, which
is probably not a good idea in most cases.  Change this by editing
the \texttt{workdir} assignment in \texttt{conf/framework.conf}.

\item Try running the build script again.  The Accelerator will
  respond by printing the resulting statistics immediately.  This is
  because all jobs have been build previously, and all
  \texttt{build()} calls will return links to existing jobs without
  executing any code.  Thus, there is full tracking of each job's
  result as well as input source data.  Changing one of the jobs or
  adding new jobs will not cause earlier jobs to be rebuilt.  It is
  easy to see how this speeds up development while minimising the risk
  of mixing up results, code, and data.  Try change something in, for
  example, \texttt{perf/a\_example\_histogram.py}, run again and see
  what happens!
\end{enumerate}


\subsection*{Changing the Dataset Size}
By default, the size of the example is 10 million lines, partitioned
into 10 datasets of one million lines each.  Changing the
\texttt{if}-statement early in \texttt{example\_perf/automata.py} from
\pyFalse to \pyTrue will expand the example 100 times, to 1.000
million lines, by creating 100 datasets of 10 million lines each.
Please note that this may take significant time (hours, depending on
hardware) and about 140GB disk space.



\clearpage
\subsection*{Results and Discussion}
The results from running on a fast\footnote{Quad ``Intel(R) Xeon(R)
  CPU E7-8867 v4 @ 2.40GHz'', 1TB RAM, and SSD storage} computer
is shown below
\begin{snugshade}
\begin{verbatim}
operation                       exec time         rows/s

csvexport                         140.405      7,122,267

reimport total                    865.740      1,155,081
   csvimport                      793.695      1,259,930
   type                            72.045     13,880,208

sum
  small number                      0.900  1,110,709,359
  small integer                     0.776  1,288,260,784
  large number                      2.158    463,384,710
  gauss number                      1.203    830,964,312
  gauss float                       1.100    909,130,792

sum positive
  small number                      2.677    373,517,282
  small integer                     2.839    352,187,637
  large number                      3.658    273,371,843
  gauss number                      3.551    281,607,174
  gauss float                       3.425    291,951,520

histogram
  number                            9.399    106,398,284
  float                             9.444    105,882,626

find string                         2.662    375,653,405

Total test time                  1337.193

Example size is 1,000,000,000 lines.
Number of slices is 139.
\end{verbatim}
\end{snugshade}

On a high level, it seems we can
\begin{itemize}
\item Sum almost one billion ($10^9$) 64-bit floats per second.  (Or
  almost 1.3 billion 32-bit ints per second.)
\item Create a binned histogram of 100 million floats or integers per
  second.
\item Find the number of strings (out of one billion) containing a
  specified substring in less than 3 seconds.
\end{itemize}
All this assuming that the data is imported by the Accelerator first.
Importing the 79GiB CSV file into a six column dataset takes less than
15 minutes (866 seconds).  Furthermore, this import is only carried
out once.  New or modified analysis jobs will use the existing import
job.  So, for example, if we change the binning of a histogram, it
will take less than 10 seconds to get the new result.



\clearpage
\section{Example:  Miscellaneous Functions}
The \texttt{example\_misc} directory contains some minimal examples
illustrating interesting topics.  This directory will likely be
expanded over time.
\subsection*{Sujobtest}
This example builds a job that itself builds a subjob.  The subjob
creates a small dataset.  Now, this dataset is not available to the
build script, since subjobs are not ``seen'' by the \texttt{urd}
object.  (We can verify this by adding
\begin{python}
print(urd.joblist.pretty)
\end{python}
at the end of the build script, and see that the subjob is not
included in the list.)  The situation is solved by linking the
subjob's dataset to the job that issued the subjob build.  This is
done using the
\begin{python}
  Dataset.link_to_here()
\end{python}
function.

The example is run by typing
\begin{shell}
./automatarunner automata_misc_subjobtest
\end{shell}
Don't forget to have a look at the output in both the \textsl{server}
terminal too.


\subsection*{Jsontest}
This example illustrates how the \texttt{json} dataset type can be
used to store more complex objects like \texttt{dict}s, \texttt{list}s
and combinations thereof.

\begin{shell}
./automatarunner automata_misc_jsontest
\end{shell}









\clearpage
\section{Example:  Dataset Operations}

This example shows how to create datasets and dataset chains, export
datasets to CSV (Comma Separated Values) files, import datasets from
CSV-files, append columns to datasets, and more.

\subsection*{Running the Example}
The example is located in the \texttt{example1} directory.  Here is a
list of steps showing how to run it:

\begin{enumerate}
\item Clone and setup the \texttt{project\_skeleton} as described
  earlier in this document
\begin{shell}
git clone https://github.com/eBay/accelerator-project_skeleton.git
cd accelerator-project_skeleton
./init.py
\end{shell}

\item Now we can start the Accelerator.  To start the server type
  \begin{shell}
cd accelerator
./daemon.py
  \end{shell}
  This terminal is now our \textsl{server} terminal, displaying the
  Accelerator's \texttt{stdout} and \texttt{stderr}.
  
\item Next, open a second terminal emulator for the \textsl{client}
  side.  The Accelerator team prefers using GNU \texttt{screen}, but any
  terminal emulator like \texttt{xterm} is fine.

  In this second terminal (green), \texttt{cd} to the
  \texttt{accelerator} directory
\begin{shell2}
cd accelerator
\end{shell2}
Since the installation is local (no administration privileges used),
using virtual environments, we need to set up the virtual environment
in this terminal like this
\begin{shell2}
source ../venv/py3/bin/activate
\end{shell2}
Now we can run the the build script.
\begin{shell2}
./automatarunner example1
\end{shell2}
\end{enumerate}



\subsection*{Looking at the Output}
The build script for this session is
\texttt{example1/automata\_example1.py}.  Please have a look at this
code with one eye while looking at the output with the other.  Here is the output:
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
example1.automata_example1
        -  example1_create_dataset                        DONE  TEST-0  0.4 seconds
        -  example1_create_dataset                        DONE  TEST-1  0.4 seconds
        -  example1_create_dataset                        DONE  TEST-2  0.5 seconds
        -  example1_create_dataset                        DONE  TEST-3  0.4 seconds
        -  example1_create_dataset                        DONE  TEST-4  0.4 seconds
        -  csvexport                                      DONE  TEST-5  0.2 seconds
\textsl{Exported file stored in "/home/ab/accelerator/workdirs/TEST/TEST-5/random.tsv"}
\end{Verbatim}
\end{snugshade}
\noindent This shows that five \texttt{example1\_create\_dataset} jobs have been
run, and their jobids are \texttt{TEST-0} to \texttt{TEST-4}.  By
looking at the code, we see that the \texttt{csvexport} job is
exporting the \textsl{last} (\texttt{TEST-4}) dataset to a CSV file on
disk.  (The whole dataset chain could be exported to CSV too by simply
changing an input parameter to \texttt{csvimport}.) The location of
this file is printed to \texttt{stdout} as well.  We move on to
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
        -  csvimport                                      DONE  TEST-6  0.2 seconds
        -  dataset_type                                   DONE  TEST-7  0.2 seconds
\end{Verbatim}
\end{snugshade}
\noindent Here, we've \textsl{imported} the CSV-file we just created.  Note
that an import types the data to \texttt{bytes}, so we issue an
\texttt{dataset\_type} job that does proper typing of the data.  Next,
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
        -  example1_calc_average                          DONE  TEST-8  0.0 seconds
\textsl{Column rint:  sum=-220338.000000, length=100000, average=-2.203380}
        -  example1_calc_average                          DONE  TEST-9  0.0 seconds
\textsl{Column rflt:  sum=50109.285978, length=100000, average=0.501093}
\end{Verbatim}
\end{snugshade}
\noindent there is a loop iterating over the columns of the dataset in
\texttt{TEST-7}.  In each iteration, it will compute the average of
the values of the column and print it to \texttt{stdout}.  Finally,
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
        -  example1_add_column                            DONE  TEST-10  0.2 seconds
        -  csvexport                                      DONE  TEST-11  0.3 seconds
\end{Verbatim}
\end{snugshade}
\noindent appends a new column to the \texttt{TEST-7} dataset.  This
dataset is then exported to a CSV file.  Which labels to export is
explicitly set in this case.  The build script then prints out a
pretty-printed version of what is in the current Urd list
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
\textsl{JobList(}
\textsl{   [  0]      Created_number_0 : TEST-0}
\textsl{   [  1]      Created_number_1 : TEST-1}
\textsl{   [  2]      Created_number_2 : TEST-2}
\textsl{   [  3]      Created_number_3 : TEST-3}
\textsl{   [  4]      Created_number_4 : TEST-4}
\textsl{   [  5]             csvexport : TEST-5}
\textsl{   [  6]             csvimport : TEST-6}
\textsl{   [  7]          dataset_type : TEST-7}
\textsl{   [  8] example1_calc_average : TEST-8}
\textsl{   [  9] example1_calc_average : TEST-9}
\textsl{   [ 10]   example1_add_column : TEST-10}
\textsl{   [ 11]             csvexport : TEST-11}
\textsl{)}
\end{Verbatim}
\end{snugshade}
\noindent Here we can see that the first five jobs have been given
explicit names that makes it possible to uniquely identify them.



\subsection*{A Look at the Datasets}
The \texttt{dsinfo} command is a simple tool to list the most
important aspects of a dataset.  Let's examine \texttt{TEST-4}, which
is the last dataset in a chain
\begin{shell2}
./dsinfo.py TEST-4
\end{shell2}
this results in
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
Parent: None
Hashlabel: None
Columns:
    rflt  float64
    rint  int64
2 columns
100,000 lines
Chain length 5, from TEST-0 to TEST-4
500,000 total lines
\end{Verbatim}
\end{snugshade}
\noindent which shows that we have 100.000 lines in \texttt{TEST-4}, and 500.000
lines in the chain starting at \texttt{TEST-0}.  Furthermore, it has
two columns, one floating point and one integer, and the dataset is
not hashed.

If we look at \texttt{TEST-10}, which is the dataset with a column
appended to \texttt{TEST-7}, it looks like this
\begin{snugshade}
\begin{Verbatim}[commandchars=\\\{\}]
Parent: TEST-7/default
Hashlabel: None
Columns:
    prod  number
    rflt  number
    rint  number
3 columns
100,000 lines
\end{Verbatim}
\end{snugshade}
\noindent Indeed, this dataset has a parent, \texttt{TEST-7/default}, and there
are three columns, all typed to \texttt{number}.
Similarly, it is worth looking at the imported dataset \texttt{TEST-6} too.



\subsection*{Conclusion}
This example shows how to do some important dataset operations, such
as importing data from a CSV file, exporting a dataset to a CSV file,
typing a dataset, creating a chain of datasets, iterating over a
dataset and compute something, and appending a new column to an
existing dataset.  It also shows how to find the absolute path to a
job result, how to access a dataset's column names, and more.  The
idea is that this example could provide a way to start playing with
the Accelerator and maybe use it in future projects.





\end{document}
